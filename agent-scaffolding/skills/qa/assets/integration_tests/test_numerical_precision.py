"""
QA Skill - Numerical Precision Integration Tests

Implements test template for task:
- IT-ML-02: Numerical Precision and Prediction Parity

Verifies that predictions from the C++ runtime (daimojo) match
the reference predictions from the Java H2O cluster.
"""

import json
import os
from pathlib import Path
from typing import Optional

import pytest


# =============================================================================
# Configuration
# =============================================================================

# Tolerance for floating-point comparison
PRECISION_TOLERANCE = float(os.getenv("PRECISION_TOLERANCE", "1e-6"))

# Path to reference predictions (generated by H2O Java cluster)
REFERENCE_PREDICTIONS_PATH = os.getenv(
    "REFERENCE_PREDICTIONS_PATH",
    "test_data/reference_predictions.json"
)


# =============================================================================
# IT-ML-02: Numerical Precision and Prediction Parity
# =============================================================================


class TestNumericalPrecision:
    """
    Context: Java (training) and C++ (inference) may handle 
    floating-point arithmetic differently.
    
    Risk: Prediction drift where the served model behaves 
    differently than the evaluated model.
    """
    
    @pytest.fixture
    def reference_data(self) -> dict:
        """
        Load reference dataset with known prediction scores from H2O cluster.
        
        Format:
        {
            "features": [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], ...],
            "predictions": [0.85, 0.72, ...],
            "model_id": "GBM_grid_1_AutoML_20240115",
            "h2o_version": "3.42.0.1"
        }
        """
        # For testing, create mock reference data
        return {
            "features": [
                [0.1, 0.2, 0.3, 0.4, 0.5],
                [0.5, 0.4, 0.3, 0.2, 0.1],
                [0.0, 0.0, 0.0, 0.0, 0.0],
                [1.0, 1.0, 1.0, 1.0, 1.0],
                [-0.5, 0.5, -0.5, 0.5, -0.5],
            ],
            "predictions": [
                0.8523456789012345,
                0.7234567890123456,
                0.5000000000000000,
                0.9876543210987654,
                0.6543210987654321,
            ],
            "model_id": "test_model_v1",
            "h2o_version": "3.42.0.1",
        }
    
    @pytest.fixture
    def mock_mojo_scorer(self, reference_data: dict):
        """
        Create a mock MOJO scorer that returns slightly different predictions.
        
        This simulates the C++ runtime producing floating-point values
        that may differ slightly from Java.
        """
        class MockMojoScorer:
            def __init__(self, reference: dict):
                self._reference = reference
                # Add tiny noise to simulate FP differences
                self._noise = 1e-10
            
            def predict(self, features: list[list[float]]) -> list[float]:
                # Return reference predictions with tiny FP noise
                import random
                return [
                    p + random.uniform(-self._noise, self._noise)
                    for p in self._reference["predictions"][:len(features)]
                ]
        
        return MockMojoScorer(reference_data)
    
    def test_predictions_match_within_tolerance(
        self,
        reference_data: dict,
        mock_mojo_scorer
    ):
        """
        Score dataset using MOJO and compare to reference scores.
        
        Evidence: Predictions match within tolerance of 1e-6.
        """
        features = reference_data["features"]
        expected = reference_data["predictions"]
        
        # Get predictions from C++ runtime
        actual = mock_mojo_scorer.predict(features)
        
        assert len(actual) == len(expected), (
            "Number of predictions should match"
        )
        
        for i, (exp, act) in enumerate(zip(expected, actual)):
            diff = abs(exp - act)
            assert diff < PRECISION_TOLERANCE, (
                f"Prediction {i}: expected {exp}, got {act}. "
                f"Difference {diff} exceeds tolerance {PRECISION_TOLERANCE}"
            )
    
    def test_extreme_values_precision(self):
        """
        Test precision for extreme floating-point values.
        """
        extreme_cases = [
            # Very small numbers
            {"java": 1e-300, "cpp": 1e-300 + 1e-310},
            # Very large numbers
            {"java": 1e300, "cpp": 1e300 + 1e290},
            # Numbers near zero
            {"java": 1e-15, "cpp": 1e-15 + 1e-20},
            # Negative numbers
            {"java": -0.123456789, "cpp": -0.123456789 + 1e-10},
        ]
        
        for i, case in enumerate(extreme_cases):
            diff = abs(case["java"] - case["cpp"])
            relative_diff = diff / (abs(case["java"]) + 1e-320)
            
            # For very large/small numbers, use relative tolerance
            assert relative_diff < 1e-10, (
                f"Case {i}: relative difference {relative_diff} too large"
            )
    
    def test_deterministic_predictions(self, mock_mojo_scorer):
        """
        Verify same input produces same output (determinism).
        """
        features = [[0.1, 0.2, 0.3, 0.4, 0.5]]
        
        pred_1 = mock_mojo_scorer.predict(features)[0]
        pred_2 = mock_mojo_scorer.predict(features)[0]
        pred_3 = mock_mojo_scorer.predict(features)[0]
        
        # All predictions should be identical (within FP tolerance)
        assert abs(pred_1 - pred_2) < PRECISION_TOLERANCE
        assert abs(pred_2 - pred_3) < PRECISION_TOLERANCE
    
    def test_batch_vs_single_consistency(self, mock_mojo_scorer):
        """
        Verify batch prediction matches individual predictions.
        
        Risk: Batch processing might use different code path.
        """
        features = [
            [0.1, 0.2, 0.3, 0.4, 0.5],
            [0.5, 0.4, 0.3, 0.2, 0.1],
            [0.3, 0.3, 0.3, 0.3, 0.3],
        ]
        
        # Batch prediction
        batch_preds = mock_mojo_scorer.predict(features)
        
        # Individual predictions
        single_preds = [
            mock_mojo_scorer.predict([f])[0] for f in features
        ]
        
        for i, (batch, single) in enumerate(zip(batch_preds, single_preds)):
            diff = abs(batch - single)
            assert diff < PRECISION_TOLERANCE, (
                f"Row {i}: batch={batch}, single={single}, diff={diff}"
            )


class TestPredictionRangeValidation:
    """
    Additional tests for prediction value ranges and distributions.
    """
    
    def test_probability_predictions_in_valid_range(self):
        """
        For classification models, predictions should be in [0, 1].
        """
        mock_predictions = [0.0, 0.25, 0.5, 0.75, 1.0, 0.999, 0.001]
        
        for pred in mock_predictions:
            assert 0.0 <= pred <= 1.0, (
                f"Probability {pred} outside valid range [0, 1]"
            )
    
    def test_regression_predictions_finite(self):
        """
        For regression models, predictions should be finite numbers.
        """
        import math
        
        mock_predictions = [100.5, -50.3, 0.0, 1e10, -1e10]
        
        for pred in mock_predictions:
            assert math.isfinite(pred), f"Prediction {pred} is not finite"


class TestCrossEnvironmentParity:
    """
    Tests for verifying parity across different execution environments.
    """
    
    def test_local_vs_container_predictions(self):
        """
        Compare predictions from local development vs Docker container.
        
        Note: Requires separate test runs in each environment.
        """
        # Reference predictions (from one environment)
        local_predictions = [0.85, 0.72, 0.91, 0.63]
        
        # Predictions from other environment (simulate)
        container_predictions = [0.85, 0.72, 0.91, 0.63]
        
        for i, (local, container) in enumerate(
            zip(local_predictions, container_predictions)
        ):
            diff = abs(local - container)
            assert diff < PRECISION_TOLERANCE, (
                f"Environment parity issue at index {i}: "
                f"local={local}, container={container}"
            )
    
    def test_cpu_vs_gpu_predictions(self):
        """
        If using GPU acceleration, verify parity with CPU predictions.
        
        Note: GPU floating-point ops may produce slightly different results.
        """
        # This is a placeholder for GPU-specific testing
        # Actual implementation depends on whether H2O MOJO uses GPU
        
        cpu_predictions = [0.8523456789]
        gpu_predictions = [0.8523456790]  # Slightly different
        
        gpu_tolerance = 1e-8  # Slightly more relaxed for GPU
        
        for cpu, gpu in zip(cpu_predictions, gpu_predictions):
            diff = abs(cpu - gpu)
            assert diff < gpu_tolerance, (
                f"CPU/GPU parity issue: cpu={cpu}, gpu={gpu}"
            )

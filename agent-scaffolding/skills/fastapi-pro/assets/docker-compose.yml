# =============================================================================
# Docker Compose - Converged MLOps Topology
# =============================================================================
#
# Physical View (4+1 Model):
# - postgres-db: State Manager (PostgreSQL 15+)
# - redis-broker: Cache & Message Broker
# - mage-scheduler: Orchestration Control Plane
# - mage-worker: Pipeline Execution Workers (scalable)
# - h2o-ai: Distributed ML Compute
# - fastapi-inference: High-Concurrency Serving
# - nginx-gateway: Reverse Proxy (only exposed service)
#
# Network Isolation:
# - backend-net: Internal communication only
# - Only nginx-gateway exposed to host
#
# Usage:
#   docker-compose up -d
#   Open http://localhost for API
#   Open http://localhost:6789 for Mage UI (dev only)
# =============================================================================

version: '3.8'

services:
  # ---------------------------------------------------------------------------
  # State Manager - PostgreSQL 15+
  # Handles: Mage state, model registry, prediction logs, drift events
  # ---------------------------------------------------------------------------
  postgres-db:
    image: postgres:15
    container_name: mlops-postgres
    hostname: postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-mlops}
    ports:
      - "5432:5432"  # Dev only - remove in production
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./warehouse/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks:
      - backend-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G

  # ---------------------------------------------------------------------------
  # Redis - Cache & Celery Broker
  # Handles: Prediction cache, Celery task queue
  # ---------------------------------------------------------------------------
  redis-broker:
    image: redis:7-alpine
    container_name: mlops-redis
    hostname: redis
    ports:
      - "6379:6379"  # Dev only
    volumes:
      - redis-data:/data
    networks:
      - backend-net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G

  # ---------------------------------------------------------------------------
  # Mage Scheduler - Orchestration Control Plane
  # Handles: DAG scheduling, UI, API
  # ---------------------------------------------------------------------------
  mage-scheduler:
    build:
      context: .
      dockerfile: docker/Dockerfile.mage
    container_name: mlops-mage-scheduler
    hostname: mage
    ports:
      - "6789:6789"  # Dev only - route through nginx in prod
    environment:
      PROJECT_NAME: mlops_project
      MAGE_DATABASE_CONNECTION_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-mlops}
      H2O_URL: http://h2o-ai:54321
      DATA_EXCHANGE_PATH: /data/exchange
      MODEL_OUTPUT_PATH: /models
      CELERY_BROKER_URL: redis://redis:6379/0
      FASTAPI_URL: http://fastapi:8000
      RELOAD_TOKEN: ${RELOAD_TOKEN:-dev-token}
    volumes:
      - ./mage_pipeline:/home/src/mlops_project
      - shared-data:/data
      - model-store:/models
    depends_on:
      postgres-db:
        condition: service_healthy
      redis-broker:
        condition: service_healthy
    networks:
      - backend-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6789/api/status"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ---------------------------------------------------------------------------
  # Mage Worker - Pipeline Execution (Celery)
  # Handles: Block execution, training jobs
  # Scalable: --scale mage-worker=N
  # ---------------------------------------------------------------------------
  mage-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.mage
    command: celery -A mage_ai.orchestration.executor worker --loglevel=info
    environment:
      PROJECT_NAME: mlops_project
      H2O_URL: http://h2o-ai:54321
      DATA_EXCHANGE_PATH: /data/exchange
      MODEL_OUTPUT_PATH: /models
      CELERY_BROKER_URL: redis://redis:6379/0
    volumes:
      - ./mage_pipeline:/home/src/mlops_project
      - shared-data:/data
      - model-store:/models
    depends_on:
      - mage-scheduler
      - redis-broker
    networks:
      - backend-net
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '2'
          memory: 4G

  # ---------------------------------------------------------------------------
  # H2O Cluster - Distributed ML Compute
  # Handles: AutoML training, model scoring
  # ---------------------------------------------------------------------------
  h2o-ai:
    image: h2oai/h2o-open-source-k8s:latest
    container_name: mlops-h2o
    hostname: h2o-ai
    ports:
      - "54321:54321"  # Dev only
    environment:
      JAVA_OPTS: -Xmx${H2O_XMX:-8g} -Xms${H2O_XMS:-4g}
    volumes:
      - shared-data:/data
      - model-store:/models
    networks:
      - backend-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:54321/3/About"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 45s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 10G
        reservations:
          memory: 8G

  # ---------------------------------------------------------------------------
  # FastAPI Inference - High-Concurrency Serving
  # Handles: Real-time predictions, model hot-swap
  # ---------------------------------------------------------------------------
  fastapi-inference:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    container_name: mlops-fastapi
    hostname: fastapi
    environment:
      MODEL_PATH: /models/production/model.mojo
      GENMODEL_JAR: /models/production/h2o-genmodel.jar
      MODEL_VERSION: ${MODEL_VERSION:-latest}
      REDIS_URL: redis://redis:6379/0
      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-mlops}
      INFERENCE_WORKERS: 4
      RELOAD_TOKEN: ${RELOAD_TOKEN:-dev-token}
    volumes:
      - model-store:/models:ro
    depends_on:
      - postgres-db
      - redis-broker
    networks:
      - backend-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '2'
          memory: 2G

  # ---------------------------------------------------------------------------
  # Nginx Gateway - Reverse Proxy
  # ONLY externally exposed service in production
  # ---------------------------------------------------------------------------
  nginx-gateway:
    image: nginx:alpine
    container_name: mlops-gateway
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - fastapi-inference
    networks:
      - backend-net
    profiles:
      - production

networks:
  backend-net:
    driver: bridge
    name: mlops-backend

volumes:
  pgdata:
    driver: local
  redis-data:
    driver: local
  shared-data:
    driver: local
  model-store:
    driver: local

# =============================================================================
# Docker Compose - Converged MLOps Topology (Production Template)
# =============================================================================
#
# 4+1 Architectural View Model - Physical View Implementation
#
# Services:
# - postgres-db: State Manager (PostgreSQL 15+ with pg_partman)
# - redis-broker: Cache & Celery Message Broker
# - mage-scheduler: Orchestration Control Plane (isolated)
# - mage-worker: Pipeline Execution Workers (Celery, scalable)
# - h2o-ai: Distributed ML Compute (Split-Memory: 70% Heap / 30% Native)
# - fastapi-inference: High-Concurrency Serving (C++ MOJO Runtime)
# - nginx-gateway: Reverse Proxy with SSL (only exposed service)
#
# Network Topology:
# - backend-net: Internal communication only (zero-trust)
# - frontend-net: Ingress network for nginx gateway
#
# Usage:
#   Development:  docker-compose up -d
#   Production:   docker-compose --profile production up -d
#   Scale Workers: docker-compose up -d --scale mage-worker=4
#
# PHY-01-01: Network isolation
# PHY-01-02: PostgreSQL production tuning
# PHY-01-04: H2O split-memory configuration
# =============================================================================

version: '3.8'

services:
  # ---------------------------------------------------------------------------
  # State Manager - PostgreSQL 15+
  # PHY-01-02: Production tuning with shared_buffers, WAL optimization
  # LOG-01-02: Enables pg_partman for time-series partitioning
  # ---------------------------------------------------------------------------
  postgres-db:
    image: postgres:15-alpine
    container_name: mlops-postgres
    hostname: postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-mlops}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-mlops_secure}
      POSTGRES_DB: ${POSTGRES_DB:-mlops}
      # Production tuning
      POSTGRES_INITDB_ARGS: "--data-checksums"
    ports:
      - "5432:5432"  # Dev only - remove in production profile
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./warehouse/init.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
      - ./warehouse/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    command: >
      postgres
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c max_wal_size=2GB
      -c checkpoint_completion_target=0.9
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=16MB
      -c maintenance_work_mem=128MB
      -c shared_preload_libraries='pg_partman_bgw'
    networks:
      - backend-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-mlops} -d ${POSTGRES_DB:-mlops}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          memory: 2G

  # ---------------------------------------------------------------------------
  # Redis - Cache & Celery Broker
  # PROC-02-02: Look-aside cache for inference
  # PROC-01-03: Celery task broker
  # ---------------------------------------------------------------------------
  redis-broker:
    image: redis:7-alpine
    container_name: mlops-redis
    hostname: redis
    command: >
      redis-server
      --appendonly yes
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"  # Dev only
    volumes:
      - redis-data:/data
    networks:
      - backend-net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G

  # ---------------------------------------------------------------------------
  # Mage Scheduler - Orchestration Control Plane (Isolated)
  # PROC-01-03: Separated from workers to prevent crash propagation
  # ---------------------------------------------------------------------------
  mage-scheduler:
    build:
      context: .
      dockerfile: docker/Dockerfile.mage
    container_name: mlops-mage-scheduler
    hostname: mage
    ports:
      - "6789:6789"  # Dev only - route through nginx in prod
    environment:
      PROJECT_NAME: mlops_project
      MAGE_DATABASE_CONNECTION_URL: postgresql://${POSTGRES_USER:-mlops}:${POSTGRES_PASSWORD:-mlops_secure}@postgres:5432/${POSTGRES_DB:-mlops}
      H2O_URL: http://h2o-ai:54321
      DATA_EXCHANGE_PATH: /data/exchange
      MODEL_OUTPUT_PATH: /models
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      FASTAPI_URL: http://fastapi-inference:8000
      RELOAD_TOKEN: ${RELOAD_TOKEN:-dev-token}
    volumes:
      - ./mage_pipeline:/home/src/mlops_project
      - shared-data:/data
      - model-store:/models
    depends_on:
      postgres-db:
        condition: service_healthy
      redis-broker:
        condition: service_healthy
    networks:
      - backend-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6789/api/status"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G

  # ---------------------------------------------------------------------------
  # Mage Worker - Pipeline Execution (Celery)
  # PROC-01-03: Isolated execution prevents scheduler crashes
  # Scalable: --scale mage-worker=N
  # ---------------------------------------------------------------------------
  mage-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.mage
    command: celery -A mage_ai.orchestration.executor worker --loglevel=info --concurrency=2
    environment:
      PROJECT_NAME: mlops_project
      MAGE_DATABASE_CONNECTION_URL: postgresql://${POSTGRES_USER:-mlops}:${POSTGRES_PASSWORD:-mlops_secure}@postgres:5432/${POSTGRES_DB:-mlops}
      H2O_URL: http://h2o-ai:54321
      DATA_EXCHANGE_PATH: /data/exchange
      MODEL_OUTPUT_PATH: /models
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
    volumes:
      - ./mage_pipeline:/home/src/mlops_project
      - shared-data:/data
      - model-store:/models
    depends_on:
      - mage-scheduler
      - redis-broker
    networks:
      - backend-net
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          memory: 2G

  # ---------------------------------------------------------------------------
  # H2O Cluster - Distributed ML Compute
  # PHY-01-04: Split-Memory Strategy (70% Heap / 30% Native)
  # XMX set to 70% of container memory to prevent OOM kills
  # ---------------------------------------------------------------------------
  h2o-ai:
    image: h2oai/h2o-open-source-k8s:3.46.0.1
    container_name: mlops-h2o
    hostname: h2o-ai
    ports:
      - "54321:54321"  # Dev only
    environment:
      # PHY-01-04: Split-Memory Allocation
      # Container limit: 12GB, Heap: 8GB (70%), Native: 4GB (30%)
      JAVA_OPTS: >-
        -Xmx${H2O_XMX:-8g}
        -Xms${H2O_XMS:-4g}
        -XX:+UseG1GC
        -XX:MaxGCPauseMillis=200
        -XX:+HeapDumpOnOutOfMemoryError
        -XX:HeapDumpPath=/data/heapdump.hprof
    volumes:
      - shared-data:/data
      - model-store:/models
    networks:
      - backend-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:54321/3/About"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 45s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 12G
        reservations:
          memory: 8G

  # ---------------------------------------------------------------------------
  # FastAPI Inference - High-Concurrency Serving
  # DEV-01-02: C++ MOJO Runtime (no JDK bloat)
  # PROC-02-01: Thread pool offloading for CPU-bound inference
  # ---------------------------------------------------------------------------
  fastapi-inference:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    container_name: mlops-fastapi
    hostname: fastapi-inference
    environment:
      MODEL_PATH: /models/production/model.mojo
      MODEL_VERSION: ${MODEL_VERSION:-latest}
      REDIS_URL: redis://redis:6379/0
      DATABASE_URL: postgresql://${POSTGRES_USER:-mlops}:${POSTGRES_PASSWORD:-mlops_secure}@postgres:5432/${POSTGRES_DB:-mlops}
      INFERENCE_WORKERS: ${INFERENCE_WORKERS:-4}
      RELOAD_TOKEN: ${RELOAD_TOKEN:-dev-token}
      # Thread pool size for CPU-bound inference
      THREAD_POOL_SIZE: ${THREAD_POOL_SIZE:-8}
    volumes:
      - model-store:/models:ro
    depends_on:
      postgres-db:
        condition: service_healthy
      redis-broker:
        condition: service_healthy
    networks:
      - backend-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '2'
          memory: 2G

  # ---------------------------------------------------------------------------
  # Nginx Gateway - Reverse Proxy with SSL
  # PHY-01-03: SSL termination, WebSocket support
  # ONLY externally exposed service in production
  # ---------------------------------------------------------------------------
  nginx-gateway:
    image: nginx:1.25-alpine
    container_name: mlops-gateway
    hostname: nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./docker/ssl:/etc/nginx/ssl:ro
    depends_on:
      - fastapi-inference
    networks:
      - frontend-net
      - backend-net
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - production

# =============================================================================
# Networks - Zero Trust Architecture
# PHY-01-01: Network isolation between frontend and backend
# =============================================================================
networks:
  backend-net:
    driver: bridge
    name: mlops-backend
    internal: true  # No external access
  frontend-net:
    driver: bridge
    name: mlops-frontend

# =============================================================================
# Volumes - Persistent Storage
# =============================================================================
volumes:
  pgdata:
    driver: local
  redis-data:
    driver: local
  shared-data:
    driver: local
  model-store:
    driver: local
